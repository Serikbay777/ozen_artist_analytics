import os
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from openai import APIConnectionError, APIError
import logging

logger = logging.getLogger(__name__)

class LLMManager:
    def __init__(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Alem.ai API —Å –º–æ–¥–µ–ª—å—é Qwen3
        self.api_key = os.getenv("ALEMAI_API_QWEN3_KEY")
        if not self.api_key:
            raise ValueError("ALEMAI_API_QWEN3_KEY not found in environment variables")
        
        # –ü–æ–ª—É—á–∞–µ–º base_url –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
        base_url = os.getenv("ALEMAI_BASE_URL", "https://llm.alem.ai/v1")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ChatOpenAI —Å Alem.ai endpoint
        # LangChain's ChatOpenAI —Å–æ–≤–º–µ—Å—Ç–∏–º —Å OpenAI-like API
        llm_kwargs = {
            "model": "qwen3",  # –ú–æ–¥–µ–ª—å Qwen3 –Ω–∞ Alem.ai
            "temperature": 0,
            "api_key": self.api_key,
            "base_url": base_url,
            "timeout": 30.0,  # –¢–∞–π–º–∞—É—Ç –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            "max_retries": 2,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
        }
            
        self.llm = ChatOpenAI(**llm_kwargs)
        logger.info(f"‚úÖ LLMManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –º–æ–¥–µ–ª—å—é qwen3 (Alem.ai)")

    def invoke(self, prompt: ChatPromptTemplate, **kwargs) -> str:
        try:
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ —Å–æ–æ–±—â–µ–Ω–∏—è
            messages = prompt.format_messages(**kwargs)
            
            logger.info(f"  ‚Üí –í—ã–∑–æ–≤ LLM (–º–æ–¥–µ–ª—å: {self.llm.model_name})")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
            logger.info("  üì§ –ó–∞–ø—Ä–æ—Å –∫ LLM:")
            for i, msg in enumerate(messages):
                role = msg.__class__.__name__.replace("Message", "")
                content = msg.content
                # –û–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
                if len(content) > 500:
                    content_preview = content[:250] + "\n...\n" + content[-250:]
                else:
                    content_preview = content
                logger.info(f"    [{i+1}] {role}:")
                for line in content_preview.split('\n'):
                    logger.info(f"      {line}")
            
            # –í—ã–∑—ã–≤–∞–µ–º OpenAI —á–µ—Ä–µ–∑ LangChain
            response = self.llm.invoke(messages)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            logger.info("  üì• –û—Ç–≤–µ—Ç –æ—Ç LLM:")
            response_content = response.content
            if len(response_content) > 1000:
                response_preview = response_content[:500] + "\n...\n" + response_content[-500:]
            else:
                response_preview = response_content
            for line in response_preview.split('\n'):
                logger.info(f"    {line}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ—Ç–≤–µ—Ç–∞
            return response.content
                
        except APIConnectionError as e:
            raise RuntimeError(
                f"Failed to connect to Alem.ai API. Please check:\n"
                f"1. Your internet connection\n"
                f"2. DNS settings\n"
                f"3. Firewall/proxy settings\n"
                f"4. API endpoint: {self.llm.openai_api_base}\n"
                f"Original error: {str(e)}"
            )
        except APIError as e:
            raise RuntimeError(f"Alem.ai API error: {str(e)}")
        except Exception as e:
            raise RuntimeError(f"Error generating answer with Alem.ai (Qwen3): {str(e)}")