"""
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ CSV –æ—Ç—á–µ—Ç—ã –æ—Ç Believe –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª
"""

import pandas as pd
import glob
import os
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞ –≤ path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

print("üöÄ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ CSV —Ñ–∞–π–ª–æ–≤ Believe")
print("=" * 60)

# –ü—É—Ç–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
data_dir = '/Users/nuraliserikbay/Desktop/codes/music_analyzer_agent/data'
output_dir = '/Users/nuraliserikbay/Desktop/codes/music_analyzer_agent/data/processed'

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –µ—Å–ª–∏ –Ω–µ—Ç
os.makedirs(output_dir, exist_ok=True)

print(f"üìÅ –ü–∞–ø–∫–∞ —Å –¥–∞–Ω–Ω—ã–º–∏: {data_dir}")
print(f"üìÅ –ü–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {output_dir}")

# –ò—â–µ–º –≤—Å–µ CSV —Ñ–∞–π–ª—ã (–≤ –ø–∞–ø–∫–µ data –∏ –ø–æ–¥–ø–∞–ø–∫–∞—Ö)
csv_pattern = os.path.join(data_dir, '**', '*.csv')
csv_files = glob.glob(csv_pattern, recursive=True)

# –ò—Å–∫–ª—é—á–∞–µ–º –ø–∞–ø–∫—É processed
csv_files = [f for f in csv_files if 'processed' not in f]

if len(csv_files) == 0:
    print("\n‚ùå CSV —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
    print(f"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å: {data_dir}")
    print(f"   –ü–æ–ª–æ–∂–∏—Ç–µ CSV —Ñ–∞–π–ª—ã –≤: music_analyzer/data/")
    sys.exit(1)

print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(csv_files)} —Ñ–∞–π–ª–æ–≤:")
for file in csv_files:
    size_kb = os.path.getsize(file) / 1024
    rel_path = os.path.relpath(file, project_root)
    print(f"   - {rel_path} ({size_kb:.1f} KB)")

print("\n" + "=" * 60)
print("üìñ –ß–∏—Ç–∞—é —Ñ–∞–π–ª—ã...")

# –ß–∏—Ç–∞–µ–º –≤—Å–µ CSV
all_dataframes = []
errors = []

for i, file in enumerate(csv_files, 1):
    filename = os.path.basename(file)
    print(f"   [{i}/{len(csv_files)}] {filename}...", end=' ')
    
    try:
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —á—Ç–µ–Ω–∏—è
        try:
            df = pd.read_csv(file, sep=';', encoding='utf-8')
        except:
            try:
                df = pd.read_csv(file, sep=',', encoding='utf-8')
            except:
                df = pd.read_csv(file, sep=';', encoding='latin1')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        df['source_file'] = filename
        
        all_dataframes.append(df)
        print(f"‚úÖ ({len(df):,} —Å—Ç—Ä–æ–∫)")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        errors.append({'file': filename, 'error': str(e)})

if len(all_dataframes) == 0:
    print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞!")
    sys.exit(1)

# –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ
print("\n" + "=" * 60)
print("üîó –û–±—ä–µ–¥–∏–Ω—è—é –¥–∞–Ω–Ω—ã–µ...")

merged = pd.concat(all_dataframes, ignore_index=True)
print(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ: {len(merged):,} —Å—Ç—Ä–æ–∫ √ó {len(merged.columns)} –∫–æ–ª–æ–Ω–æ–∫")

# –ß–∏—Å—Ç–∏–º –¥–∞–Ω–Ω—ã–µ
print("\nüßπ –ß–∏—â—É –¥–∞–Ω–Ω—ã–µ...")

# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø–æ–ª—è
numeric_cols = [
    '–û–±—â–∏–π –¥–æ—Ö–æ–¥', 
    '–°—É–º–º–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è', 
    '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', 
    '–¶–µ–Ω–∞ –∑–∞ –µ–¥–∏–Ω–∏—Ü—É',
    '–ê–≤—Ç–æ—Ä—Å–∫–∏–µ –æ—Ç—á–∏—Å–ª–µ–Ω–∏—è (–º–µ—Ö–∞–Ω–∏–∫–∞)',
    '–°—Ç–∞–≤–∫–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è'
]

for col in numeric_cols:
    if col in merged.columns:
        merged[col] = pd.to_numeric(
            merged[col].astype(str).str.replace(',', '.'),
            errors='coerce'
        )

# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–∞—Ç—ã
date_cols = ['–ú–µ—Å—è—Ü –æ—Ç—á–µ—Ç–∞', '–ú–µ—Å—è—Ü –ø—Ä–æ–¥–∞–∂–∏']
for col in date_cols:
    if col in merged.columns:
        merged[col] = pd.to_datetime(merged[col], errors='coerce')

print("‚úÖ –î–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã")

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
print("\n" + "=" * 60)
print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
print("=" * 60)

if '–ú–µ—Å—è—Ü –æ—Ç—á–µ—Ç–∞' in merged.columns:
    min_date = merged['–ú–µ—Å—è—Ü –æ—Ç—á–µ—Ç–∞'].min()
    max_date = merged['–ú–µ—Å—è—Ü –æ—Ç—á–µ—Ç–∞'].max()
    print(f"\nüìÖ –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"   –û—Ç: {min_date}")
    print(f"   –î–æ: {max_date}")

if '–°—É–º–º–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è' in merged.columns:
    total_revenue = merged['–°—É–º–º–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è'].sum()
    currency = merged['–í–∞–ª—é—Ç–∞'].iloc[0] if '–í–∞–ª—é—Ç–∞' in merged.columns else 'EUR'
    print(f"\nüí∞ –í—ã—Ä—É—á–∫–∞:")
    print(f"   –í—Å–µ–≥–æ: {currency}{total_revenue:,.2f}")

if '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ' in merged.columns:
    total_streams = merged['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'].sum()
    print(f"\nüéµ –°—Ç—Ä–∏–º—ã:")
    print(f"   –í—Å–µ–≥–æ: {total_streams:,.0f}")

if '–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å' in merged.columns:
    print(f"\nüé§ –ö–æ–Ω—Ç–µ–Ω—Ç:")
    print(f"   –ê—Ä—Ç–∏—Å—Ç–æ–≤: {merged['–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å'].nunique()}")
    
if '–ù–∞–∑–≤–∞–Ω–∏–µ —Ç—Ä–µ–∫–∞' in merged.columns:
    print(f"   –¢—Ä–µ–∫–æ–≤: {merged['–ù–∞–∑–≤–∞–Ω–∏–µ —Ç—Ä–µ–∫–∞'].nunique()}")

if '–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞' in merged.columns:
    print(f"   –ü–ª–∞—Ç—Ñ–æ—Ä–º: {merged['–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞'].nunique()}")

if '—Å—Ç—Ä–∞–Ω–∞ / —Ä–µ–≥–∏–æ–Ω' in merged.columns:
    print(f"   –°—Ç—Ä–∞–Ω: {merged['—Å—Ç—Ä–∞–Ω–∞ / —Ä–µ–≥–∏–æ–Ω'].nunique()}")

# –¢–æ–ø-5 –∞—Ä—Ç–∏—Å—Ç–æ–≤
if '–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å' in merged.columns and '–°—É–º–º–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è' in merged.columns:
    print(f"\nüèÜ –¢–æ–ø-5 –∞—Ä—Ç–∏—Å—Ç–æ–≤:")
    top = merged.groupby('–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å')['–°—É–º–º–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è'].sum().sort_values(ascending=False).head(5)
    for artist, revenue in top.items():
        print(f"   {artist}: {currency}{revenue:,.2f}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
print("\n" + "=" * 60)
print("üíæ –°–æ—Ö—Ä–∞–Ω—è—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")

# CSV
csv_path = os.path.join(output_dir, 'all_believe_data.csv')
merged.to_csv(csv_path, index=False, encoding='utf-8')
csv_size_mb = os.path.getsize(csv_path) / 1024 / 1024
print(f"   ‚úÖ CSV: {csv_size_mb:.1f} MB")

# Pickle (–±—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
pkl_path = os.path.join(output_dir, 'all_believe_data.pkl')
merged.to_pickle(pkl_path)
pkl_size_mb = os.path.getsize(pkl_path) / 1024 / 1024
print(f"   ‚úÖ Pickle: {pkl_size_mb:.1f} MB")

# Parquet (—Å–∂–∞—Ç—ã–π)
try:
    parquet_path = os.path.join(output_dir, 'all_believe_data.parquet')
    merged.to_parquet(parquet_path, index=False)
    parquet_size_mb = os.path.getsize(parquet_path) / 1024 / 1024
    print(f"   ‚úÖ Parquet: {parquet_size_mb:.1f} MB")
except:
    print(f"   ‚ö†Ô∏è  Parquet: –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω pyarrow")

# Summary —Ñ–∞–π–ª
summary_path = os.path.join(output_dir, 'data_summary.txt')
with open(summary_path, 'w', encoding='utf-8') as f:
    f.write("BELIEVE ANALYTICS - DATA SUMMARY\n")
    f.write("=" * 60 + "\n\n")
    f.write(f"–§–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(csv_files)}\n")
    f.write(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(merged):,}\n")
    f.write(f"–ö–æ–ª–æ–Ω–æ–∫: {len(merged.columns)}\n\n")
    
    f.write("–ö–û–õ–û–ù–ö–ò:\n")
    for col in merged.columns:
        f.write(f"  - {col}\n")
    
    if errors:
        f.write(f"\n\n–û–®–ò–ë–ö–ò ({len(errors)}):\n")
        for error in errors:
            f.write(f"  - {error['file']}: {error['error']}\n")

print(f"   ‚úÖ Summary: data_summary.txt")

# –§–∏–Ω–∞–ª
print("\n" + "=" * 60)
print("üéâ –ì–û–¢–û–í–û!")
print("=" * 60)
print("\n–í–∞—à–∏ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:")
print(f"   üìÅ {os.path.relpath(output_dir, project_root)}/")
print(f"      - all_believe_data.csv")
print(f"      - all_believe_data.pkl")
print(f"      - all_believe_data.parquet")
print(f"      - data_summary.txt")

print("\nüí° –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ Python:")
print("   import pandas as pd")
print(f"   df = pd.read_pickle('{os.path.join('app', 'data', 'processed', 'all_believe_data.pkl')}')")
print("   print(df.head())")

if errors:
    print(f"\n‚ö†Ô∏è  –í–Ω–∏–º–∞–Ω–∏–µ: {len(errors)} —Ñ–∞–π–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏ (—Å–º. data_summary.txt)")

print("\n‚ú® –ì–æ—Ç–æ–≤–æ –∫ –∞–Ω–∞–ª–∏–∑—É! üöÄ\n")